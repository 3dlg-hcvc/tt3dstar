<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>TT3DSTAR</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="TT3DSTAR" />
<meta name="author" content="default" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="TT3DSTAR" />
<meta property="og:description" content="TT3DSTAR" />
<meta property="og:site_name" content="TT3DSTAR" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="TT3DSTAR" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","author":{"@type":"Person","name":"default"},"description":"TT3DSTAR","headline":"TT3DSTAR","name":"TT3DSTAR","url":"/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="./assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="TT3DSTAR" />
</head>
<body>

    <!--<header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">TT3DSTAR</a></div>
</header>
-->

    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home"><h1 id="text-to-3d-shape-generation-paper-list">Text-to-3D Shape Generation Paper List</h1>

<p><strong>Eurographics STAR 2024</strong></p>

<p><a href="https://hanhung.github.io/">Han-Hung Lee<sup>1</sup></a>, 
<a href="https://msavva.github.io/">Manolis Savva<sup>1</sup></a> and 
<a href="https://angelxuanchang.github.io/">Angel Xuan Chang<sup>1,2</sup></a></p>

<p><sup>1</sup> Simon Fraser University <sup>2</sup> Canada-CIFAR AI Chair, Amii</p>

<p><a href="https://arxiv.org/"><img src="https://img.shields.io/badge/arXiv-xxx-b31b1b.svg" height="22.5" /></a></p>

<p><img src="./assets/images/text-to-3D-overview.png" alt="alt text" /></p>

<h2 id="abstract">Abstract</h2>
<blockquote>
  <p>Recent years have seen an explosion of work and interest in text-to-3D shape generation. Much of the progress is driven by
advances in 3D representations, large-scale pretraining and representation learning for text and image data enabling generative
AI models, and differentiable rendering. Computational systems that can perform text-to-3D shape generation have captivated
the popular imagination as they enable non-expert users to easily create 3D content directly from text. However, there are
still many limitations and challenges remaining in this problem space. In this state-of-the-art report, we provide a survey of
the underlying technology and methods enabling text-to-3D shape generation to summarize the background literature. We then
derive a systematic categorization of recent work on text-to-3D shape generation based on the type of supervision data required.
Finally, we discuss limitations of the existing categories of methods, and delineate promising directions for future work.</p>
</blockquote>

<p>We list the commonly used datasets used to train these methods <a href="#DATASETS">here</a>.</p>

<p>The methods are divided into four families as shown in the table below, namely: 1) <a href="#3DPT">Paired Text to 3D (3DPT)</a>; 2) <a href="#3DUT">Unpaired 3D Data (3DUT)</a>; 3) <a href="#NO3D">Text-to-3D without 3D data (NO3D)</a>; and 4) <a href="#HYBRID3D">Hybrid3D</a>.</p>

<p><img src="./assets/images/classification.png" alt="alt text" /></p>

<p>Finally, we include works focused on generating <a href="#MULTIOBJGEN">multi-object 3D scenes</a>, <a href="#EDIT3D">editing of 3D shapes</a> and <a href="#EVAL">evaluation</a> of text-to-3d methods.</p>

<h2 id="datasets"><a id="DATASETS"></a>Datasets</h2>

<h3 id="3d">3D</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1512.03012">ShapeNet: An Information-Rich 3D Model Repository</a>,<br />
Chang et al., Arxiv 2015<br />
<a href="https://shapenet.org/">Website</a></li>
  <li><a href="https://arxiv.org/abs/2110.06199">ABO: Dataset and Benchmarks for Real-World 3D Object Understanding</a>,<br />
Collins et al., CVPR 2022<br />
<a href="https://amazon-berkeley-objects.s3.amazonaws.com/index.html">Website</a></li>
  <li><a href="https://arxiv.org/abs/2212.08051">Objaverse: A Universe of Annotated 3D Objects</a>,<br />
Deitke et al., CVPR 2023<br />
<a href="https://objaverse.allenai.org/">Website</a></li>
  <li><a href="https://arxiv.org/abs/2307.05663">Objaverse-XL: A Universe of 10M+ 3D Objects</a>,<br />
Deitke et al., NeurIPS 2023<br />
<a href="https://objaverse.allenai.org/">Website</a></li>
</ul>

<h3 id="text-3d">Text-3D</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1803.08495">Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings</a>,<br />
Chen et al., Arxiv 2018<br />
<a href="https://github.com/kchen92/text2shape/"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/1905.02925">ShapeGlot: Learning Language for Shape Differentiation</a>,<br />
Achlioptas et al., ICCV 2019<br />
<a href="https://github.com/optas/shapeglot"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Achlioptas_ShapeTalk_A_Language_Dataset_and_Framework_for_3D_Shape_Edits_CVPR_2023_paper.pdf">ShapeTalk: A Language Dataset and Framework for 3D Shape Edits and Deformations</a>,<br />
Achlioptas et al., CVPR 2023<br />
<a href="https://github.com/optas/changeit3d"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2305.10764">OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding</a>,<br />
Liu et al.,NeurIPS 2023<br />
<a href="https://github.com/Colin97/OpenShape_code"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2306.07279">Scalable 3D Captioning with Pretrained Models</a>,<br />
Luo et al., NeurIPS 2023<br />
<a href="https://github.com/crockwell/Cap3D"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
</ul>

<p><img src="./assets/images/overview-models.png" alt="alt text" /></p>

<h2 id="paired-text-to-3d-3dpt"><a id="3DPT"></a>Paired Text to 3D (3DPT)</h2>

<p><img src="./assets/images/3DPT-table.png" alt="alt text" /></p>

<ul>
  <li><a href="https://arxiv.org/abs/1803.08495">Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings</a>,<br />
Chen et al., Arxiv 2018<br />
<a href="https://github.com/kchen92/text2shape/"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2203.14622">Towards Implicit Text-Guided 3D Shape Generation</a>,<br />
Liu et al., CVPR 2022<br />
<a href="https://github.com/liuzhengzhe/Towards-Implicit-Text-Guided-Shape-Generation"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
</ul>

<p><img src="./assets/images/auto-diff.png" alt="alt text" /></p>

<h3 id="autoregressive-prior">Autoregressive Prior</h3>

<ul>
  <li><a href="https://arxiv.org/abs/2203.09516">AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation</a>,<br />
Mittal et al., CVPR 2022<br />
<a href="https://github.com/yccyenchicheng/AutoSDF"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2207.09446">ShapeCrafter: A Recursive Text-Conditioned 3D Shape Generation Model</a>,<br />
Fu et al., NeurIPS 2022<br />
<a href="https://github.com/FreddieRao/ShapeCrafter"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
</ul>

<h3 id="diffusion-prior">Diffusion Prior</h3>

<ul>
  <li><a href="https://arxiv.org/abs/2212.04493">SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation</a>,<br />
Cheng et al., CVPR 2023<br />
<a href="https://github.com/yccyenchicheng/SDFusion"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2212.03293">Diffusion-SDF: Text-to-Shape via Voxelized Diffusion</a><br />
Li et al., CVPR 2023,<br />
<a href="https://github.com/ttlmh/Diffusion-SDF"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2303.10406">3DQD: Generalized Deep 3D Shape Prior via Part-Discretized Diffusion Process</a>,<br />
Li et al., CVPR 2023<br />
<a href="https://github.com/colorful-liyu/3DQD"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2305.02463">Shap-E: Generating Conditional 3D Implicit Functions</a>,<br />
Jun et al., Arxiv 2023<br />
<a href="https://github.com/openai/shap-e"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2306.17115">Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation</a>,<br />
Zhao et al., NeurIPS 2023<br />
<a href="https://github.com/NeuralCarver/Michelangelo"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
</ul>

<h3 id="structure-aware">Structure Aware</h3>
<ul>
  <li><a href="https://www.yongliangyang.net/docs/shapescaffolder_iccv23.pdf">ShapeScaffolder: Structure-Aware 3D Shape Generation from Text</a>,<br />
Tian et al., ICCV 2023</li>
  <li><a href="https://arxiv.org/abs/2303.12236">SALAD: Part-Level Latent Diffusion for 3D Shape Generation and Manipulation</a>,<br />
Koo et al., ICCV 2023<br />
<a href="https://github.com/KAIST-Visual-AI-Group/SALAD"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2212.12952">Neural Shape Compiler: A Unified Framework for Transforming between Text, Point Cloud, and Program</a>,<br />
Luo et al., Arxiv 2022<br />
<a href="https://github.com/tiangeluo/ShapeCompiler"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
</ul>

<h2 id="unpaired-3d-data-3dut"><a id="3DUT"></a>Unpaired 3D Data (3DUT)</h2>

<p><img src="./assets/images/3DUT-table.png" alt="alt text" /></p>

<ul>
  <li><a href="https://arxiv.org/abs/2110.02624">CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation</a>,<br />
Sanghi et al., CVPR 2022<br />
<a href="https://github.com/AutodeskAILab/Clip-Forge"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2211.01427">CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Natural Language</a>,<br />
Sanghi et al., CVPR 2023</li>
  <li><a href="https://arxiv.org/abs/2209.04145">ISS: Image as Stepping Stone for Text-Guided 3D Shape Generation</a>,<br />
Liu et al., ICLR 2023<br />
<a href="https://github.com/liuzhengzhe/DreamStone-ISS"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2303.13273">TAPS3D: Text-Guided 3D Textured Shape Generation from Pseudo Supervision</a>,<br />
Wei et al., CVPR 2023<br />
<a href="https://github.com/plusmultiply/TAPS3D"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
</ul>

<h2 id="text-to-3d-without-3d-data-no3d"><a id="NO3D"></a>Text-to-3D without 3D data (NO3D)</h2>

<h3 id="unsupervised-clip-guidance">Unsupervised CLIP Guidance</h3>

<p><img src="./assets/images/clip-table.png" alt="alt text" /></p>

<ul>
  <li><a href="https://arxiv.org/abs/2112.01455">Zero-Shot Text-Guided Object Generation with Dream Fields</a>,<br />
Jain et al., CVPR 2022<br />
<a href="https://github.com/google-research/google-research/tree/master/dreamfields"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2203.13333">CLIP-Mesh: Generating textured meshes from text using pretrained image-text models</a>,<br />
Khalid et al., SIGGRAPH Asia 2022<br />
<a href="https://github.com/NasirKhalid24/CLIP-Mesh"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2209.15172">Understanding Pure CLIP Guidance for Voxel Grid NeRF Models</a>,<br />
Lee et al., Arxiv 2022<br />
<a href="https://github.com/hanhung/PureCLIPNeRF"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2212.14704">Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models</a>,<br />
Xu et al., CVPR 2023<br />
<a href="https://bluestyle97.github.io/dream3d/index.html"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
</ul>

<h3 id="unsupervised-diffusion-guidance">Unsupervised Diffusion Guidance</h3>

<p><img src="./assets/images/diffusion-table.png" alt="alt text" /></p>

<h4 id="loss-formulation">Loss Formulation</h4>
<ul>
  <li><a href="https://arxiv.org/abs/2209.14988">DreamFusion: Text-to-3D using 2D Diffusion</a>,<br />
Poole et al., ICLR 2023</li>
  <li><a href="https://arxiv.org/abs/2212.00774">Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation</a>,<br />
Wang et al., CVPR 2023<br />
<a href="https://github.com/pals-ttic/sjc"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2305.16213">ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation</a>,<br />
Wang et al., NeurIPS 2023<br />
<a href="https://github.com/thu-ml/prolificdreamer"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
</ul>

<h4 id="3d-representation-improvements">3D Representation Improvements</h4>
<ul>
  <li><a href="https://arxiv.org/abs/2211.10440">Magic3D: High-Resolution Text-to-3D Content Creation</a>,<br />
Lin et al., CVPR 2023</li>
  <li><a href="https://arxiv.org/abs/2304.12439">TextMesh: Generation of Realistic 3D Meshes From Text Prompts</a>,<br />
Tsalicoglou et al., 3DV 2024</li>
  <li><a href="https://arxiv.org/abs/2303.13873">Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation</a>,<br />
Chen et al., ICCV 2023<br />
<a href="https://github.com/Gorilla-Lab-SCUT/Fantasia3D"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2309.16653">DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation</a>,<br />
Tang et al., ICLR 2024<br />
<a href="https://github.com/dreamgaussian/dreamgaussian"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2309.16585">Text-to-3D using Gaussian Splatting</a>,<br />
Chen et al., CVPR 2024<br />
<a href="https://github.com/gsgen3d/gsgen"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2310.08529">GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models</a>,<br />
Yi et al., CVPR 2024<br />
<a href="https://github.com/hustvl/GaussianDreamer"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
</ul>

<h4 id="janus-problem-mitigation">Janus Problem Mitigation</h4>
<ul>
  <li><a href="https://arxiv.org/abs/2303.15413">Debiasing Scores and Prompts of 2D Diffusion for View-consistent Text-to-3D Generation</a>,<br />
Hong et al., NeurIPS 2023<br />
<a href="https://github.com/SusungHong/Debiased-Score-Distillation-Sampling"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2303.07937">Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation</a>,<br />
Seo et al., ICLR 2024<br />
<a href="https://github.com/KU-CVLAB/3DFuse?tab=readme-ov-file"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2304.04968">Re-imagine the Negative Prompt Algorithm: Transform 2D Diffusion into 3D, alleviate Janus problem and Beyond</a>,<br />
Armandpour et al., Arxiv 2023<br />
<a href="https://github.com/Perp-Neg/Perp-Neg-stablediffusion"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
</ul>

<h4 id="generative-modeling">Generative Modeling</h4>
<ul>
  <li><a href="https://arxiv.org/abs/2306.07349">ATT3D: Amortized Text-to-3D Object Synthesis</a>,<br />
Lorraine et al., ICCV 2023</li>
  <li><a href="https://arxiv.org/abs/2311.08403">Instant3D: Instant Text-to-3D Generation</a>,<br />
Li et al., Arxiv 2023<br />
<a href="https://github.com/ming1993li/Instant3DCodes"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
</ul>

<h4 id="further-reading">Further Reading</h4>
<ul>
  <li><a href="https://arxiv.org/abs/2311.11284">LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching</a>,<br />
Liang et al., CVPR 2024<br />
<a href="https://github.com/EnVision-Research/LucidDreamer"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2310.17590">Noise-Free Score Distillation</a>,<br />
Katzir et al., Arxiv 2023<br />
<a href="https://github.com/orenkatzir/nfsd"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2401.00604">SteinDreamer: Variance Reduction for Text-to-3D Score Distillation via Stein Identity</a>,<br />
Wang et al., Arxiv 2023<br />
<a href="https://github.com/VITA-Group/SteinDreamer"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2310.19415">Text-to-3D with Classifier Score Distillation</a>,<br />
Yu et al., ICLR 2024<br />
<a href="https://github.com/CVMI-Lab/Classifier-Score-Distillation"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2401.00909">Taming Mode Collapse in Score Distillation for Text-to-3D Generation</a>,<br />
Wang et al., Arxiv 2023<br />
<a href="https://github.com/VITA-Group/3D-Mode-Collapse"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2312.09305">Stable Score Distillation for High-Quality 3D Generation</a>,<br />
Tang et al., Arxiv 2023</li>
  <li><a href="https://arxiv.org/abs/2311.17082">DreamPropeller: Supercharge Text-to-3D Generation with Parallel Sampling</a>,<br />
Zhou et al., Arxiv 2023<br />
<a href="https://github.com/alexzhou907/DreamPropeller"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2310.12474">Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping</a>,<br />
Pan et al., ICLR 2024 2023<br />
<a href="https://github.com/fudan-zvg/PGC-3D"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2401.09050">Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation with Deterministic Sampling Prior</a>,<br />
Wu et al., Arxiv 2024</li>
  <li>Open Source Text-to-3D Re-implementations<br />
<a href="https://github.com/ashawkey/stable-dreamfusion">stable-dreamfusion</a><br />
<a href="https://github.com/threestudio-project/threestudio">threestudio</a></li>
</ul>

<h2 id="hybrid3d"><a id="HYBRID3D"></a>Hybrid3D</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2212.08751">Point-E: A System for Generating 3D Point Clouds from Complex Prompts</a>,<br />
Nichol et al., Arxiv 2022<br />
<a href="https://github.com/openai/point-e"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
</ul>

<h3 id="3d-aware-t2i">3D-aware T2I</h3>

<p><img src="./assets/images/Hybrid3D-table.png" alt="alt text" /></p>

<h4 id="text-conditioning">Text Conditioning</h4>
<ul>
  <li><a href="https://arxiv.org/abs/2308.16512">MVDream: Multi-view Diffusion for 3D Generation</a>,<br />
Shi et al., Arxiv 2023<br />
<a href="https://github.com/bytedance/MVDream"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2310.02596">SweetDreamer: Aligning Geometric Priors in 2D Diffusion for Consistent Text-to-3D</a>,<br />
Li et al., Arxiv 2023<br />
<a href="https://github.com/wyysf-98/SweetDreamer"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2311.15980">Direct2.5: Diverse Text-to-3D Generation via Multi-view 2.5D Diffusion</a>,<br />
Lu et al., Arxiv 2023</li>
  <li><a href="https://arxiv.org/abs/2312.08754">UniDream: Unifying Diffusion Priors for Relightable Text-to-3D Generation</a>,<br />
Liu et al., Arxiv 2023<br />
<a href="https://github.com/YG256Li/UniDream"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2311.06214">Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model</a>,<br />
Li et al., ICLR 2024</li>
</ul>

<h4 id="image-conditioning">Image Conditioning</h4>
<ul>
  <li><a href="https://arxiv.org/abs/2303.11328">Zero-1-to-3: Zero-shot One Image to 3D Object</a>,<br />
Liu et al., ICCV 2023<br />
<a href="https://github.com/cvlab-columbia/zero123"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2309.03453">SyncDreamer: Generating Multiview-consistent Images from a Single-view Image</a>,<br />
Liu et al., Arxiv 2023<br />
<a href="https://github.com/liuyuan-pal/SyncDreamer"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2310.15110">Zero123++: a Single Image to Consistent Multi-view Diffusion Base Model</a>,<br />
Shi et al., Arxiv 2023<br />
<a href="https://github.com/SUDO-AI-3D/zero123plus"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2310.15008">Wonder3D: Single Image to 3D using Cross-Domain Diffusion</a>,<br />
Long et al., Arxiv 2023<br />
<a href="https://github.com/xxlong0/Wonder3D"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2311.04400">LRM: Large Reconstruction Model for Single Image to 3D</a>,<br />
Hong et al., ICLR 2024</li>
  <li><a href="https://arxiv.org/abs/2306.16928">One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization</a>,<br />
Liu et al., NeurIPS 2023<br />
<a href="https://github.com/One-2-3-45/One-2-3-45"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2311.07885">One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion</a>,<br />
Liu et al., Arxiv 2023<br />
<a href="https://github.com/SUDO-AI-3D/One2345plus"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
</ul>

<h3 id="further-reading-1">Further Reading</h3>
<ul>
  <li><a href="https://arxiv.org/abs/2311.09217">DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model</a>,<br />
Xu et al., Arxiv 2023</li>
</ul>

<h2 id="multi-object-scene-generation"><a id="MULTIOBJGEN"></a>Multi Object Scene Generation</h2>

<h3 id="compositional-generation">Compositional Generation</h3>
<ul>
  <li><a href="https://arxiv.org/abs/2303.13450">Set-the-Scene: Global-Local Training for Generating Controllable NeRF Scenes</a>,<br />
Cohen-Bar et al., ICCVW 2023<br />
<a href="https://github.com/DanaCohen95/Set-the-Scene"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2303.13843">CompoNeRF: Text-guided Multi-object Compositional NeRF with Editable 3D Scene Layout</a>,<br />
Bai et al., Arxiv 2023</li>
  <li><a href="https://arxiv.org/abs/2303.12218">Compositional 3D Scene Generation using Locally Conditioned Diffusion</a>,<br />
Po et al., Arxiv 2023</li>
  <li><a href="https://arxiv.org/abs/2311.17907">CG3D: Compositional Generation for Text-to-3D via Gaussian Splatting</a>,<br />
Vilesov et al., Arxiv 2023</li>
  <li><a href="https://arxiv.org/abs/2312.00093">GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs</a>,<br />
Gao et al., CVPR 2024<br />
<a href="https://github.com/GGGHSL/GraphDreamer"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2312.08885">SceneWiz3D: Towards Text-guided 3D Scene Composition</a>,<br />
Zhang et al., Arxiv 2023<br />
<a href="https://github.com/zqh0253/SceneWiz3D"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
</ul>

<h3 id="rgbd-fusion-for-scenes">RGBD Fusion for Scenes</h3>
<ul>
  <li><a href="https://arxiv.org/abs/2302.01133">SceneScape: Text-Driven Consistent Scene Generation</a>,<br />
Fridman et al., NeurIPS 2023<br />
<a href="https://github.com/RafailFridman/SceneScape"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2303.11989">Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models</a>,<br />
HÃ¶llein et al., ICCV 2023<br />
<a href="https://github.com/lukasHoel/text2room"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2305.11588">Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields</a>,<br />
Zhang et al., TVCG 2024<br />
<a href="https://github.com/eckertzhang/Text2NeRF"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
</ul>

<h2 id="editing"><a id="EDIT3D"></a>Editing</h2>

<h3 id="shape-editing-with-clip">Shape Editing with CLIP</h3>
<ul>
  <li><a href="https://arxiv.org/abs/2112.05139">CLIP-NeRF: Text-and-Image Driven Manipulation of Neural Radiance Fields</a>,<br />
Wang et al., CVPR 2022<br />
<a href="https://github.com/cassiePython/CLIPNeRF"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2112.03221">Text2Mesh: Text-Driven Neural Stylization for Meshes</a>,<br />
Michel et al., CVPR 2022<br />
<a href="https://github.com/threedle/text2mesh"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
</ul>

<h3 id="scene-editing-with-text-to-image-models">Scene Editing with Text-to-image Models</h3>
<ul>
  <li><a href="https://arxiv.org/abs/2303.10735">SKED: Sketch-guided Text-based 3D Editing</a>,<br />
Mikaeili et al., ICCV 2023<br />
<a href="https://github.com/aryanmikaeili/SKED"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2303.12048">Vox-E: Text-guided Voxel Editing of 3D Objects</a>,<br />
Sella et al., ICCV 2023<br />
<a href="https://github.com/TAU-VAILab/Vox-E"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2303.12789">Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions</a>,<br />
Haque et al., ICCV 2023<br />
<a href="https://github.com/ayaanzhaque/instruct-nerf2nerf"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://sony.github.io/Instruct3Dto3D-doc/static/pdf/Instruct3Dto3D.pdf">Instruct 3D-to-3D: Text Instruction Guided 3D-to-3D conversion</a>,<br />
Kamata et al., Arxiv 2023</li>
  <li><a href="https://arxiv.org/abs/2305.11337">RoomDreamer: Text-Driven 3D Indoor Scene Synthesis with Coherent Geometry and Texture</a>,<br />
Song et al., Arxiv 2023</li>
</ul>

<h3 id="texturing">Texturing</h3>
<ul>
  <li><a href="https://arxiv.org/abs/2302.01721">TEXTure: Text-Guided Texturing of 3D Shapes</a>,<br />
Richardson et al., SIGGRAPH 2023<br />
<a href="https://github.com/TEXTurePaper/TEXTurePaper"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2303.11396">Text2Tex: Text-driven Texture Synthesis via Diffusion Models</a>,<br />
Chen et al., ICCV 2023<br />
<a href="https://github.com/daveredrum/Text2Tex"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
  <li><a href="https://arxiv.org/abs/2311.17261">SceneTex: High-Quality Texture Synthesis for Indoor Scenes via Diffusion Priors</a>,<br />
Chen et al., Arxiv 2023<br />
<a href="https://github.com/daveredrum/SceneTex"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
</ul>

<h2 id="evaluation"><a id="EVAL"></a>Evaluation</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2401.04092">GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation</a>,<br />
Yang et al., CVPR 2024<br />
<a href="https://github.com/3DTopia/GPTEval3D"><img src="https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&amp;logo=github&amp;logoColor=white" alt="GitHub" /></a></li>
</ul>



  </div>

      </div>
    </main>

    <!--<footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">default</li>
          <li><a class="u-email" href="mailto:default">default</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>TT3DSTAR
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
-->

  </body>

</html>
